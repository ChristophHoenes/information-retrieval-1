{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pytrec_eval\n",
    "import read_ap\n",
    "import download_ap\n",
    "import scipy.stats\n",
    "import timeit\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from tf_idf import TfIdfRetrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_METRICS = False\n",
    "\n",
    "def write_results(model, mdic):\n",
    "    results_path = \"results\"\n",
    "    if not os.path.exists(results_path):\n",
    "        os.mkdir(results_path)\n",
    "        \n",
    "    #dump metrics to JSON\n",
    "    with open(os.path.join(results_path, model+\".json\"), \"w\") as writer:\n",
    "        json.dump(mdic[\"metrics\"], writer, indent=1)\n",
    "        \n",
    "    #write file with all query-doc pairs, scores, ranks, etc.\n",
    "    f = open(os.path.join(results_path, model+\".dat\"), \"w\")\n",
    "    for qid in mdic[\"results\"]:\n",
    "        prevscore = 1e9\n",
    "        for rank, docid in enumerate(mdic[\"results\"][qid], 1):\n",
    "            score = mdic[\"results\"][qid][docid]\n",
    "            if score > prevscore:\n",
    "                f.close()\n",
    "                raise Exception(\"'results_dic' not ordered! Stopped writing results\")\n",
    "            f.write(f\"{qid} Q0 {docid} {rank} {score} STANDARD\\n\")\n",
    "            prevscore = score\n",
    "    f.close()\n",
    "    \n",
    "def perform_ttest(m1, m2, metric, models, thresh=0.05, print_res=True):\n",
    "    #if pvalue < thresh (usually 0.05), then diff is significant\n",
    "    \n",
    "    qids = [qid for qid in models[m1][\"metrics\"]]\n",
    "    scores1 = [models[m1][\"metrics\"][qid][metric] for qid in qids]\n",
    "    scores2 = [models[m2][\"metrics\"][qid][metric] for qid in qids]   \n",
    "    pvalue = scipy.stats.ttest_rel(scores1, scores2).pvalue\n",
    "    conclusion = \"significant diff\" if pvalue < thresh else \"insignificant diff\"\n",
    "    print(\"{:<25} {:<25} {:<19} {:<7} p-value = {:<5.3}\".format(m1, m2, conclusion, \"(\"+metric+\")\", pvalue))\n",
    "    return pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs already processed. Loading from disk\n"
     ]
    }
   ],
   "source": [
    "#read data\n",
    "if not LOAD_METRICS:\n",
    "    docs = read_ap.get_processed_docs()\n",
    "    qrels, queries = read_ap.read_qrels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus from disk...\n"
     ]
    }
   ],
   "source": [
    "#prepare models\n",
    "\n",
    "if not LOAD_METRICS:\n",
    "    models = {}\n",
    "\n",
    "#     models[\"TF-IDF\"]     = {\"model\": TfIdfRetrieval(docs), \"results\": {}, \"metrics\": {}}\n",
    "    models[\"LDA500\"]     = {\"model\": LDARetrieval(docs, get_model=True, \n",
    "                                                        num_topics=500, \n",
    "                                                        passes=6, \n",
    "                                                        iterations=40, \n",
    "                                                        prep_search=False), \"results\": {}, \"metrics\": {}}\n",
    "    # run by everyone individually, metrics loaded from file\n",
    "    # models[\"word2vec\"]   = {\"model\": ..., \"results\": {}, \"metrics\": {}}\n",
    "    # models[\"doc2vec\"]    = {\"model\": ..., \"results\": {}, \"metrics\": {}}\n",
    "    # models[\"LSI-BoW\"]    = {\"model\": ..., \"results\": {}, \"metrics\": {}}\n",
    "    # models[\"LSI-TF-IDF\"] = {\"model\": ..., \"results\": {}, \"metrics\": {}}\n",
    "    # models[\"LDA\"]        = {\"model\": ..., \"results\": {}, \"metrics\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run each query for each model\n",
    "\n",
    "if not LOAD_METRICS:\n",
    "    for model in models:\n",
    "        for qid in qrels: \n",
    "            query_text = queries[qid]\n",
    "            models[model][\"results\"][qid] = dict(models[model][\"model\"].search(query_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate results\n",
    "\n",
    "metrics = {'map', 'ndcg'}\n",
    "\n",
    "if not LOAD_METRICS:\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, metrics)\n",
    "\n",
    "    for model in models:\n",
    "        models[model][\"metrics\"] = evaluator.evaluate(models[model][\"results\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write results\n",
    "\n",
    "if not LOAD_METRICS:\n",
    "    for model in models:\n",
    "        write_results(model, models[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_METRICS: \n",
    "    models = {}\n",
    "    results_path = \"./results/\"\n",
    "    for fname in os.listdir(results_path):\n",
    "        if fname[-4:] == \"json\":\n",
    "            model = fname[:-5]\n",
    "            models[model] = {}\n",
    "            with open(results_path + fname, \"r\") as fp:\n",
    "                models[model][\"metrics\"] = json.load(fp)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow10                     ndcg  : 0.31 +/- 0.11\n",
      "d2v_10                    ndcg  : 0.27 +/- 0.099\n",
      "d2v_100k                  ndcg  : 0.27 +/- 0.098\n",
      "d2v_10k                   ndcg  : 0.26 +/- 0.092\n",
      "d2v_15                    ndcg  : 0.27 +/- 0.1\n",
      "d2v_20                    ndcg  : 0.27 +/- 0.1\n",
      "d2v_200k                  ndcg  : 0.27 +/- 0.098\n",
      "d2v_25k                   ndcg  : 0.27 +/- 0.1\n",
      "d2v_300                   ndcg  : 0.27 +/- 0.098\n",
      "d2v_400                   ndcg  : 0.27 +/- 0.093\n",
      "d2v_5                     ndcg  : 0.28 +/- 0.11\n",
      "d2v_500                   ndcg  : 0.27 +/- 0.096\n",
      "d2v_50k                   ndcg  : 0.27 +/- 0.099\n",
      "d2v_bestvalid_metrics_5   ndcg  : 0.28 +/- 0.11\n",
      "LDA10                     ndcg  : 0.31 +/- 0.12\n",
      "LDA100                    ndcg  : 0.39 +/- 0.17\n",
      "LDA1000                   ndcg  : 0.38 +/- 0.18\n",
      "LDA50                     ndcg  : 0.36 +/- 0.14\n",
      "LDA500                    ndcg  : 0.4 +/- 0.19\n",
      "lsi_bow10                 ndcg  : 0.31 +/- 0.11\n",
      "lsi_bow100                ndcg  : 0.41 +/- 0.17\n",
      "lsi_bow1000               ndcg  : 0.48 +/- 0.19\n",
      "lsi_bow2000               ndcg  : 0.5 +/- 0.19\n",
      "lsi_bow50                 ndcg  : 0.38 +/- 0.16\n",
      "lsi_bow500                ndcg  : 0.46 +/- 0.19\n",
      "lsi_tfidf10               ndcg  : 0.28 +/- 0.1\n",
      "lsi_tfidf100              ndcg  : 0.36 +/- 0.16\n",
      "lsi_tfidf1000             ndcg  : 0.46 +/- 0.2\n",
      "lsi_tfidf2000             ndcg  : 0.48 +/- 0.2\n",
      "lsi_tfidf50               ndcg  : 0.33 +/- 0.14\n",
      "lsi_tfidf500              ndcg  : 0.44 +/- 0.19\n",
      "TF-IDF                    ndcg  : 0.58 +/- 0.22\n",
      "bow10                     map   : 0.0059 +/- 0.013\n",
      "d2v_10                    map   : 0.0042 +/- 0.028\n",
      "d2v_100k                  map   : 0.0043 +/- 0.035\n",
      "d2v_10k                   map   : 0.0013 +/- 0.004\n",
      "d2v_15                    map   : 0.006 +/- 0.053\n",
      "d2v_20                    map   : 0.0051 +/- 0.044\n",
      "d2v_200k                  map   : 0.0037 +/- 0.02\n",
      "d2v_25k                   map   : 0.006 +/- 0.054\n",
      "d2v_300                   map   : 0.0041 +/- 0.03\n",
      "d2v_400                   map   : 0.0023 +/- 0.0098\n",
      "d2v_5                     map   : 0.0063 +/- 0.031\n",
      "d2v_500                   map   : 0.0038 +/- 0.023\n",
      "d2v_50k                   map   : 0.0047 +/- 0.04\n",
      "d2v_bestvalid_metrics_5   map   : 0.0066 +/- 0.031\n",
      "LDA10                     map   : 0.0064 +/- 0.013\n",
      "LDA100                    map   : 0.04 +/- 0.077\n",
      "LDA1000                   map   : 0.05 +/- 0.1\n",
      "LDA50                     map   : 0.02 +/- 0.038\n",
      "LDA500                    map   : 0.058 +/- 0.1\n",
      "lsi_bow10                 map   : 0.0059 +/- 0.013\n",
      "lsi_bow100                map   : 0.05 +/- 0.086\n",
      "lsi_bow1000               map   : 0.1 +/- 0.15\n",
      "lsi_bow2000               map   : 0.11 +/- 0.15\n",
      "lsi_bow50                 map   : 0.033 +/- 0.063\n",
      "lsi_bow500                map   : 0.083 +/- 0.13\n",
      "lsi_tfidf10               map   : 0.0032 +/- 0.011\n",
      "lsi_tfidf100              map   : 0.034 +/- 0.074\n",
      "lsi_tfidf1000             map   : 0.093 +/- 0.14\n",
      "lsi_tfidf2000             map   : 0.1 +/- 0.14\n",
      "lsi_tfidf50               map   : 0.019 +/- 0.053\n",
      "lsi_tfidf500              map   : 0.077 +/- 0.13\n",
      "TF-IDF                    map   : 0.22 +/- 0.21\n"
     ]
    }
   ],
   "source": [
    "#print avg metrics\n",
    "bestmodels = [\"d2v_5\", \"LDA500\", \"lsi_bow2000\", \"lsi_tfidf2000\", \"TF-IDF\", \"bow10\"]\n",
    "\n",
    "for metric in metrics:\n",
    "    for model in models:\n",
    "        res = np.array([models[model][\"metrics\"][qid][metric] for qid in models[model][\"metrics\"]])\n",
    "        string = \"{:<25} {:<6}: {:<3.2} +/- {:<3.2}\".format(model, metric, np.mean(res), np.std(res))\n",
    "        print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow10                     d2v_5                     significant diff    (ndcg)  p-value = 2.15e-06\n",
      "bow10                     LDA500                    significant diff    (ndcg)  p-value = 1.4e-14\n",
      "bow10                     lsi_bow2000               significant diff    (ndcg)  p-value = 9.2e-32\n",
      "bow10                     lsi_tfidf2000             significant diff    (ndcg)  p-value = 1e-25\n",
      "bow10                     TF-IDF                    significant diff    (ndcg)  p-value = 1.11e-33\n",
      "d2v_5                     LDA500                    significant diff    (ndcg)  p-value = 6.78e-19\n",
      "d2v_5                     lsi_bow2000               significant diff    (ndcg)  p-value = 6.1e-35\n",
      "d2v_5                     lsi_tfidf2000             significant diff    (ndcg)  p-value = 6.22e-29\n",
      "d2v_5                     TF-IDF                    significant diff    (ndcg)  p-value = 3.66e-37\n",
      "LDA500                    lsi_bow2000               significant diff    (ndcg)  p-value = 9.13e-12\n",
      "LDA500                    lsi_tfidf2000             significant diff    (ndcg)  p-value = 8.96e-07\n",
      "LDA500                    TF-IDF                    significant diff    (ndcg)  p-value = 4.77e-18\n",
      "lsi_bow2000               lsi_tfidf2000             insignificant diff  (ndcg)  p-value = 0.131\n",
      "lsi_bow2000               TF-IDF                    significant diff    (ndcg)  p-value = 2.52e-06\n",
      "lsi_tfidf2000             TF-IDF                    significant diff    (ndcg)  p-value = 5.1e-13\n"
     ]
    }
   ],
   "source": [
    "#perform t-tests\n",
    "\n",
    "ttest = {}\n",
    "for metric in metrics:\n",
    "    for model1 in models:\n",
    "        if model1 in bestmodels:\n",
    "            for model2 in models:\n",
    "                if model2 in bestmodels:\n",
    "                    if model1 != model2 and model1+\" \"+model2 not in ttest.keys() and model2+\" \"+model1 not in ttest.keys():                  \n",
    "                        ttest[model1+\" \"+model2] = {metric: perform_ttest(model1, model2, metric, models)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not LOAD_METRICS:\n",
    "#     num_topics = 500\n",
    "#     ldamodel = models[\"LDA500\"][\"model\"].model\n",
    "#     top_topics = ldamodel.top_topics(models[\"LDA500\"][\"model\"].corpus, topn=10) \n",
    "#     for i, (top, _)  in enumerate(top_topics[:5]):   \n",
    "#     print(i)\n",
    "#     for _, word in top:\n",
    "#         print(word)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvIR1",
   "language": "python",
   "name": "venvir1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
