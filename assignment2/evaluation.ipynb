{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pytrec_eval\n",
    "import read_ap\n",
    "import download_ap\n",
    "import scipy.stats\n",
    "import timeit\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from tf_idf import TfIdfRetrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_METRICS = True\n",
    "\n",
    "def write_results(model, mdic):\n",
    "    results_path = \"results\"\n",
    "    if not os.path.exists(results_path):\n",
    "        os.mkdir(results_path)\n",
    "        \n",
    "    #dump metrics to JSON\n",
    "    with open(os.path.join(results_path, model+\".json\"), \"w\") as writer:\n",
    "        json.dump(mdic[\"metrics\"], writer, indent=1)\n",
    "        \n",
    "    #write file with all query-doc pairs, scores, ranks, etc.\n",
    "    f = open(os.path.join(results_path, model+\".dat\"), \"w\")\n",
    "    for qid in mdic[\"results\"]:\n",
    "        prevscore = 1e9\n",
    "        for rank, docid in enumerate(mdic[\"results\"][qid], 1):\n",
    "            score = mdic[\"results\"][qid][docid]\n",
    "            if score > prevscore:\n",
    "                f.close()\n",
    "                raise Exception(\"'results_dic' not ordered! Stopped writing results\")\n",
    "            f.write(f\"{qid} Q0 {docid} {rank} {score} STANDARD\\n\")\n",
    "            prevscore = score\n",
    "    f.close()\n",
    "    \n",
    "def perform_ttest(m1, m2, metric, models, thresh=0.05, print_res=True):\n",
    "    #if pvalue < thresh (usually 0.05), then diff is significant\n",
    "    \n",
    "    qids = [qid for qid in models[m1][\"metrics\"]]\n",
    "    scores1 = [models[m1][\"metrics\"][qid][metric] for qid in qids]\n",
    "    scores2 = [models[m2][\"metrics\"][qid][metric] for qid in qids]   \n",
    "    pvalue = scipy.stats.ttest_rel(scores1, scores2).pvalue\n",
    "    conclusion = \"significant diff\" if pvalue < thresh else \"insignificant diff\"\n",
    "    print(\"{:<12} {:<12} {:<19} {:<7} p-value = {:<5.3}\".format(m1, m2, conclusion, \"(\"+metric+\")\", pvalue))\n",
    "    return pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "if not LOAD_METRICS:\n",
    "    docs = read_ap.get_processed_docs()\n",
    "    qrels, queries = read_ap.read_qrels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare models\n",
    "\n",
    "if not LOAD_METRICS:\n",
    "    models = {}\n",
    "\n",
    "    models[\"TF-IDF\"]     = {\"model\": TfIdfRetrieval(docs), \"results\": {}, \"metrics\": {}}\n",
    "    models[\"LDA500\"]     = {\"model\": LDARetrieval(docs, get_model=True, \n",
    "                                                        num_topics=500, \n",
    "                                                        passes=6, \n",
    "                                                        iterations=40, \n",
    "                                                        prep_search=True), \"results\": {}, \"metrics\": {}}\n",
    "    # models[\"word2vec\"]   = {\"model\": ..., \"results\": {}, \"metrics\": {}}\n",
    "    # models[\"doc2vec\"]    = {\"model\": ..., \"results\": {}, \"metrics\": {}}\n",
    "    # models[\"LSI-BoW\"]    = {\"model\": ..., \"results\": {}, \"metrics\": {}}\n",
    "    # models[\"LSI-TF-IDF\"] = {\"model\": ..., \"results\": {}, \"metrics\": {}}\n",
    "    # models[\"LDA\"]        = {\"model\": ..., \"results\": {}, \"metrics\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run each query for each model\n",
    "\n",
    "if not LOAD_METRICS:\n",
    "    for model in models:\n",
    "        for qid in qrels: \n",
    "            query_text = queries[qid]\n",
    "            models[model][\"results\"][qid] = dict(models[model][\"model\"].search(query_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate results\n",
    "\n",
    "metrics = {'map', 'ndcg'}\n",
    "\n",
    "if not LOAD_METRICS:\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, metrics)\n",
    "\n",
    "    for model in models:\n",
    "        models[model][\"metrics\"] = evaluator.evaluate(models[model][\"results\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write results\n",
    "\n",
    "if not LOAD_METRICS:\n",
    "    for model in models:\n",
    "        write_results(model, models[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_METRICS:\n",
    "    models = {\"LDA10\":{}, \"LDA50\":{}, \"LDA100\":{}, \"LDA500\":{}}\n",
    "    for model in models:\n",
    "        with open(\"./results/\" + f\"{model}.json\",\"r\") as fp:\n",
    "            models[model][\"metrics\"] = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA10        ndcg  : 0.31 +/- 0.12\n",
      "LDA50        ndcg  : 0.36 +/- 0.14\n",
      "LDA100       ndcg  : 0.39 +/- 0.17\n",
      "LDA500       ndcg  : 0.4 +/- 0.19\n",
      "LDA10        map   : 0.0064 +/- 0.013\n",
      "LDA50        map   : 0.02 +/- 0.038\n",
      "LDA100       map   : 0.04 +/- 0.077\n",
      "LDA500       map   : 0.058 +/- 0.1\n"
     ]
    }
   ],
   "source": [
    "#print avg metrics\n",
    "for metric in metrics:\n",
    "    for model in models:\n",
    "        res = np.array([models[model][\"metrics\"][qid][metric] for qid in models[model][\"metrics\"]])\n",
    "        string = \"{:<12} {:<6}: {:<3.2} +/- {:<3.2}\".format(model, metric, np.mean(res), np.std(res))\n",
    "        print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform t-tests\n",
    "\n",
    "ttest = {}\n",
    "for metric in metrics:\n",
    "    for model1 in models:\n",
    "        for model2 in models:\n",
    "#             if model1 != model2:\n",
    "            #or, to reduce redundancy:\n",
    "            if model1 != model2 and model1+\" \"+model2 not in ttest and model2+\" \"+model1 not in ttest:      \n",
    "            \n",
    "                ttest[model1+\" \"+model2] = {metric: perform_ttest(model1, model2, metric, models)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvIR1",
   "language": "python",
   "name": "venvir1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
