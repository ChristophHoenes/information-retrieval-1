{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Yke/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Yke/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tf_idf import TfIdfRetrieval\n",
    "from collections import defaultdict, Counter\n",
    "import tqdm\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "import pytrec_eval\n",
    "from tqdm import tqdm\n",
    "\n",
    "import read_ap\n",
    "import download_ap\n",
    "\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write results function\n",
    "\n",
    "def write_results(model, mdic):\n",
    "    results_path = \"results\"\n",
    "    if not os.path.exists(results_path):\n",
    "        os.mkdir(results_path)\n",
    "        \n",
    "    #dump metrics to JSON\n",
    "    with open(os.path.join(results_path, model+\".json\"), \"w\") as writer:\n",
    "        json.dump(mdic[\"metrics\"], writer, indent=1)\n",
    "        \n",
    "    #write file with all query-doc pairs, scores, ranks, etc.\n",
    "    f = open(os.path.join(results_path, model+\".dat\"), \"w\")\n",
    "    for qid in mdic[\"results\"]:\n",
    "        prevscore = 1e9\n",
    "        for rank, docid in enumerate(mdic[\"results\"][qid], 1):\n",
    "            score = mdic[\"results\"][qid][docid]\n",
    "            if score > prevscore:\n",
    "                f.close()\n",
    "                raise Exception(\"'results_dic' not ordered! Stopped writing results\")\n",
    "            f.write(f\"{qid} Q0 {docid} {rank} {score} STANDARD\\n\")\n",
    "            prevscore = score\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs already processed. Loading from disk\n"
     ]
    }
   ],
   "source": [
    "#read data\n",
    "\n",
    "docs = read_ap.get_processed_docs()\n",
    "qrels, queries = read_ap.read_qrels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare models\n",
    "\n",
    "models = {}\n",
    "\n",
    "models[\"TF-IDF\"]     = {\"model\": TfIdfRetrieval(docs), \"results\": {}, \"metrics\": {}}\n",
    "# models[\"word2vec\"]   = {\"model\": ..., \"results\": {}, \"metrics\": {}}\n",
    "# models[\"doc2vec\"]    = {\"model\": ..., \"results\": {}, \"metrics\": {}}\n",
    "# models[\"LSI-BoW\"]    = {\"model\": ..., \"results\": {}, \"metrics\": {}}\n",
    "# models[\"LSI-TF-IDF\"] = {\"model\": ..., \"results\": {}, \"metrics\": {}}\n",
    "# models[\"LDA\"]        = {\"model\": ..., \"results\": {}, \"metrics\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run each model for each query\n",
    "\n",
    "for qid in qrels: \n",
    "    query_text = queries[qid]\n",
    "\n",
    "    #this might be slightly different for each model\n",
    "    models[\"TF-IDF\"][\"results\"][qid] = dict(models[\"TF-IDF\"][\"model\"].search(query_text))\n",
    "    # models[\"word2vec\"][\"results\"]   = ...\n",
    "    # models[\"doc2vec\"][\"results\"]    = ...\n",
    "    # models[\"LSI-BoW\"][\"results\"]    = ...\n",
    "    # models[\"LSI-TF-IDF\"][\"results\"] = ...\n",
    "    # models[\"LDA\"][\"results\"]        = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate results\n",
    "\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'map', 'ndcg'})\n",
    "\n",
    "for model in models:\n",
    "    models[model][\"metrics\"] = evaluator.evaluate(models[model][\"results\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write results\n",
    "\n",
    "for model in models:\n",
    "    write_results(model, models[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvIR1",
   "language": "python",
   "name": "venvir1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
